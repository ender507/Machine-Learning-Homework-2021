# 机器学习与数据挖掘Assignment5：

# 基于变长前缀编码的相似性检索  

>姓名：507
>
>学号：18340057
>
>班级：计算机科学二班

## 一、 背景和动机  

当今社会中，信息和数据正在飞速产生。这些大量的数据可以被广泛使用，但往往数据量庞大且杂乱。我们需要将这些数据转换成有用的信息和知识，获取的信息和知识可以广泛用于各种应用。相似性搜索用于检索大量数据中的部分内容。给定数据项$q$，我们希望能在庞大的数据库中检索出与其类似的其他数据，这就是相似性搜索的主要工作。

相似性检索有许多实际应用，如类似或近似重复的文档搜索，可用于查找类似的文章与论文查重；图片的相似性检索可以用于搜索套图等。好的相似性检索算法能够在数据量巨大的数据库中找出类似的数据，在繁杂的信息中提取有效的内容。

-----------

## 二、 当前解决该问题的主要方法

### 2.1 朴素算法

相似性检索最简单朴素的方法是将查询项$q$与数据库中所有数据项逐一比较，然后检索相似度最高的数据项。如果数据库中的数据量为$N$，则该算法的时间复杂度为$O(N)$，随着数据库中数据量的增大而线性增长。现当今，计算机网络中产生的数据量巨大，这种方法显然是不适用的。

### 2.2 局部敏感哈希LSH

我们使得每个数据通过一个哈希函数的处理，可以分别被分配到一个桶中。通过建立哈希表，我们就能很快知道每个数据的位置。当新增数据时，我们只需要用哈希函数处理该数据项，就能知道该数据项属于的桶。而哈希函数计算较为高效，一般认为是$O(1)$的时间复杂度就能完成。而局部敏感哈希希望，类似的数据项通过哈希映射到桶中时，往往被分配到同一个桶或者相近的桶中；相差较大的数据项通过映射后，被分配到相距较远或相差较大的桶中。如此一来，在给定数据项$q$时，我们只需要将其通过哈希函数处理得到其属于的桶，然后将属于该桶或者属于邻近的桶的数据项作为结果返回即可。问题在于如何现在的问题是如何得到局部敏感的哈希函数，而哈希函数的设计往往依赖于具体的应用场景和数据表示方法。

### 2.4 监督哈希

监督哈希的基本思想鼓励具有相同标签的数据输出相似的代码，不同标签的数据输出不同的代码。这一种方法有很多不同的实现与算法，如：训练一个神经网络分类器，对倒数第二层的输出进行二进制化，以生成二进制哈希码；训练一个神经网络分类器，输入为两张图片，在两张图片相似时标签为1，否则为0进行训练等等，总的来说，就是训练分类器对图片相似与否进行判断。

### 2.5 无监督哈希

无监督哈希的基本思想是在未标记数据实例上训练生成模型，用数据的潜在表示构造哈希码。如，训练一个朴素变分自编码器实例，将数据的平均近似后验分布作为其哈希编码；采用伯努利先验和后验，使得模型可以直接输出二值表示等等。

-------------

## 三、 我构想的模型与算法  

考虑到数据的“相似性”定义是人为的，我希望能有一种更好的方式，能够精确到不同的相似性程度。比如，在某种“相似性”下，猫A和猫B是相似的，因为它们都是猫，而猫和狗不是相似的；而在更加宏观的“相似性”下，猫和狗是相似的，因为它们都是动物，而猫和卡车是不不相似的。所以，依据“相似性”范围的不同，我们希望可以由相似性检索得到不同范围的结果，需要有一种表示方法，能够体现不同程度的相似性：“猫和猫”的相似性比“猫和狗”更强，而“猫和狗”之间的相似性比“猫和卡车”之间更强，因此我想到了前缀编码表示。

依据相似性检索的要求，每个数据项应该都要映射为一个二进制数。在上面的方法中，这个二进制数的不同位的地位往往是相同的。而前缀编码要求位数越靠前的位表示更加宽泛的相似。比如，猫的编码为“1111”，狗的编码为“1110”，卡车的编码为“010”，我们只需要看他们的二进制数的前几位是否相同就能判断他们的相似度。如猫和猫，他们的相似度为4，因为它们的编码都为“1111”，而猫和狗的相似度为3，因为猫的二进制编码是“1111”，狗是“1110”，它们的前缀只有“111”共三位是相同的。同样，猫和卡车的相似度为0，因为猫以1开头而卡车以0开头，它们之间找不到相同的前缀码。两个数据项越相似，它们的编码具有的相同前缀就越长。编码中越前面的数位表示越宏观的相似度，而越后面的编码表示越精确的相似。

想要有不同层次的相似度，就需要对不同层次的相似度进行定义。下面以[cifar10数据集](http://www.cs.toronto.edu/~kriz/cifar.html)为例进行解释说明。cifar10数据集有10类的图片数据，分别为飞机、汽车、鸟、猫、鹿、狗、青蛙、马、船、卡车。显然，它们有着不同层次的相似度。按照我的划分方法，可以将它们的相似度用以下方式划分：

<img src="pic\\1.png" style="zoom:50%;" />

在上图中，圆圈内表示划分方法，圆圈引申出的左箭头表示不满足圆圈条件，右箭头表示满足圆圈条件。显然，这些相似性是具有层次性的。如，鸟和猫都是动物，在这个层面上它们是相似的，而猫是哺乳动物但鸟不是，从这个更细分的层面上来说，二者又是不相似的。通过这些不同层次的详细性，我们就能知道给定的数据项和其他数据项在何种程度上相似。如果左箭头给定编码0，而右箭头给定编码1，则从根节点开始到叶子节点，每个数据项都可以被分配到一个二进制编码。飞机、汽车、鸟、猫、鹿、狗、青蛙、马、船、卡车被分配到的编码分别为：`"001","011","101","1111","1101","1110","100","1100","000","010"`。可以看到，越相近的数据，具有相同的前缀码就越长。而且，在这种结构下，不同数据的编码长度可能不同，所以这是一种变长编码。

所以，有了上面的树状结构，即相似性的层次结构后，就能给每个数据项分配编码了。现在的问题就变为了，如何构建上述的层次结构。考虑到每个节点都对数据进行了一次二分类，我们可以训练多个分类器，每个节点用一个分类器表示。分类器输出0或1，表明该数据项的编码的下一位为0还是1，然后将数据送到下一层次的相应的分类器处，继续进行分类，获得编码的下一位结果，以此类推直到数据项到达叶子节点。分类器可以一开始由训练数据进行监督学习训练好，在给定新的数据项时只需要用训练好的分类器依次进行推理，就能得到该数据项的二进制编码表示。通过比较编码的相同的前缀长度，就能知道这些数据的相似性。同样，这些数据存放在桶中，但这些桶不是并列的桶，而是也存在这层次结构。大桶中又分成多个小桶，小桶中有着更小的桶，共用越小的桶表示数据项越相近。

以上就是基于变长前缀编码的相似性检索的模型。其具体步骤如下：

1. 依据数据类型，规定不同层次的“相似性”定义。这里的相似性是二值的，即结果只能为0或1，从而形成上述的树形结构
2. 为每个相似性比较，即树的每个非叶子节点，设计一个分类器
3. 对上述分类器进行训练

在实际的相似性检索过程中，就能依据相似性的层次对相似的数据进行检索：

1. 给定数据项$q$，从根节点开始进行分类器分类，依据结果为0或1进入左儿子节点或右儿子节点
2. 重复上述步骤直到达到需要的相似性层次
3. 以当前节点为根节点，该子树所有叶子节点的数据项就是需要返回的相似数据

-------

## 四、 实验结果及分析  

同样，以cifar10数据集为例，我使用`python`对该数据集进行了编码与分类。其中，分类器使用了`pytorch`库。每个分类器是一个CNN模型，其定义如下：

```python
# 定义卷积神经网络
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)
        self.fc4 = nn.Linear(10, 2)
        self.relu = nn.ReLU()
        self.softmax = nn.Softmax()

    def forward(self, x):
        x = self.pool(self.relu(self.conv1(x)))
        x = self.pool(self.relu(self.conv2(x)))
        x = x.view(-1, 16 * 5 * 5)
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.relu(self.fc3(x))
        x = self.softmax(self.fc4(x))
        return x

```

读入数据集后，按照标签进行分类。相同标签的样本被划分到同一个列表中，如第3类样本存在`trainloader[3]`中。因为每个分类器需要进行分类的样本类型并不完全相同，我们就需要将这些样本按照类型区分开来，针对不同的分类器，分配对应的训练样本和标签。

```python
train_images = np.load("cifar10_train_images.npy")
train_labels = np.load("cifar10_train_labels.npy")
trainloader = [[] for i in range(10)]    # trainloader列表分别存放10个不同类的样本的data
for i in range(train_images.shape[0]):
    data = torch.Tensor(train_images[i]).reshape((3,32,32))
    label = train_labels[i]
    trainloader[label].append([data, label])
    # 用DataLoader类保存数据
for i in range(10):
    trainloader[i] = torch.utils.data.DataLoader(trainloader[i], batch_size=1000, shuffle=False)
```

按照上面的分类方法，一共需要9个分类器来实现该模型：

```python
# 创建CNN实例
net_ = Net()#是否是动物
net0_ = Net()#是否在地上跑
net00_ = Net()#是否是飞机
net01_ =Net()#是否是汽车
net1_ = Net()#是否是哺乳动物
net10_ = Net()#是否是鸟
net11_ = Net()#是否可以做家养宠物
net110_ = Net()#是否是鹿
net111_ = Net()#是否是猫
# 训练分类器
net_ = trainNet(net_, trainloader, [0,1,8,9])
net0_ = trainNet(net0_, [trainloader[0],trainloader[1],trainloader[8],trainloader[9]],[0,8])
net00_ = trainNet(net00_, [trainloader[0],trainloader[8]],[8])
net1_ = trainNet(net1_, [trainloader[2],trainloader[3],trainloader[4],trainloader[5],
                         trainloader[6],trainloader[7]],[2,6])
net10_ = trainNet(net10_, [trainloader[2],trainloader[6]],[6])
net11_ = trainNet(net11_, [trainloader[3],trainloader[4],trainloader[5],trainloader[7]],[4,7])
net110_ = trainNet(net110_, [trainloader[4],trainloader[7]],[7])
net111_ = trainNet(net111_, [trainloader[3],trainloader[5]],[5])
```

需要说明的是，在训练过程中，每个分类器的标签并不是原来的0~9。因为每个分类器是一个二值分类器，最终的标签只能是0或者1。在调用`trainNet`函数时，我们需要将分类器、该分类器可能遇到类别的训练数据，以及需要被判负类的标签传递过去。如，训练第一个分类器`net_`时，需要传递所有的训练样本，这些样本的类别为0~9，而该分类器是判断样本是否为动物的，因此需要被判负类的样本标签应该为`[0,1,8,9]`。而`net0_`分类器是对已经确认不是动物的样本进行分类，所以传递训练样本时，只需要传递非动物类型的样本，然后给定相应的负类标签。

`trainNet`函数的实现如下所示。在调用函数时，给定了判断为负类的标签的列表`zero_labels`，因此，在获得标签时，需要对标签重新赋值：如果标签在`zero_labels`中出现了，则标签改为0，否则为1。这样一来，就能保证训练的分类器是一个二值分类器：

```python
def trainNet(net, sub_trainloader, zero_labels):
    criterion = nn.CrossEntropyLoss()           # 损失函数
    optimizer = optim.SGD(net.parameters(), lr=0.001)   # 批梯度下降，学习率
    for each_trainloader in sub_trainloader:
        for epoch in range(100):        # 学习次数
            running_loss = 0.0          # 损失函数值
            for i, data in enumerate(each_trainloader, 0):
                inputs, labels = data
                optimizer.zero_grad()   # 将参数梯度清零
                outputs = net(inputs)   # 前向传播
                for i in range(labels.shape[0]):
                    if labels[i] in zero_labels:
                        labels[i] = 0
                    else:
                        labels[i] = 1
                loss = criterion(outputs, labels)   # 计算损失
                loss.backward()         # 反向传播
                optimizer.step()        # 更新梯度
    print('fin')
    return net
```

训练好所有分类器后，就能开始进行相似性的检索。随机读入一条测试数据：

```python
# 读入测试集，随机选择一条数据
test_images = np.load("cifar10_test_images.npy")
sample = torch.Tensor(test_images[random.randint(0,test_images.shape[0]-1)].reshape((1,3,32,32)))
```

然后按照上面的树形结构对该样本进行编码：

```python
# 将输入样例进行编码
    code = ""
    while True:
        outputs = net(sample)
        _, predicted = torch.max(outputs.data, 1)
        code = code + str(np.array(predicted)[0])
        if int(np.array(predicted)[0]) == 0:
            if net == net_:
                net = net0_
            elif net == net0_:
                net = net00_
            elif net == net1_:
                net = net10_
            elif net == net11_:
                net = net110_
            else:
                break
        else:
            if net == net_:
                net = net1_
            elif net == net1_:
                net = net11_
            elif net == net11_:
                net = net111_
            else:
                break
    print('该样本的编码为',code)
```

最终就能得到该样本的二进制码。之后就能通过取该码的前面的连续二进制数位，确定不同相似度的样本，从而进行返回（为简单起见，程序中只是将这些相似样本属于的桶给返回了）：

```python
    length = len(code)
    sim = input('请输入相似度，范围为0到'+str(length)+':')
    sim = int(sim)
    print('匹配到的相似编码为：')
    for each in encoded_label:
        if len(each)>=sim and each[:sim] == code[:sim]:
                print(each)
```

程序运行结果如下所示：

<img src="pic\\2.png" style="zoom:50%;" />

可以看到，该模型可以成功地对未知的新输入样本进行编码，并按照编码找到相应的类似样本。与其他相似性搜索不同的是，这种模型能够自己定制相似性检索的“相似度”，从而在不同的相似性规模上匹配到相应的类似样本。

---------

## 五、 结论

经过测试和比较，该模型有以下的优点和缺点。

优点：

1. 支持不同规模的相似性匹配。这是我提出该模型的初衷，相似性并不是两个样本“相似与否”，而是两个样本之间可以进行相似程度的比较。在相似性搜索时，可以定义更加严格的或者更加宽泛的相似性，从而使得相似性搜索更加灵活
2. 变长编码在某种程度上可以节省空间。在上面的cifar10的例子中，要给10类不同的样本进行编码，至少需要4个二进制位，也就是说这10类样本需要$10\times4=40$位来表示。而在上面的例子中，有的二进制码为3位，有的为4位，10类表示的总位数为34。这和如何定义每一个分类器有关。如果分类器定义得当，可以节省二进制数表示的空间：

```python
# 十个类分别为：0飞机、1汽车、2鸟、3猫、4鹿、5狗、6青蛙、7马、8船、9卡车
encoded_label = ["001","011","101","1111","1101","1110","100","1100","000","010"]
```

3. 前缀编码更加容易进行相似度比较。对于LSP处理后两个二进制数是否相似，则一般是将这两个数的每一位看做一个特征，从而得到两个向量，计算这两个向量的距离，如欧式距离，从而得到二者的差距，作为判断是否相似的依据。如果码长为$n$则比较的时间复杂度为$O(n)$而前缀编码只需从第一位开始一次往后比较，一旦出现不同的编码，比较可以马上停止。也就是说，其时间复杂度与相似度$s$是正相关的，时间复杂度为$O(r)$。因为相似度$r$定义为相同前缀码的长度，故显然有$r\leq n$，因此相似度比较时该模型有更好的效率。

缺点：

1. 每个分类器的分类依据需要人为定义。一方面，这使得在大数据应用下该模型难以应用，因为各种复杂的数据很难进行人为定义的分类。cifar10只有10类数据，就要进行9次二分类，更何况更复杂、更庞大的数据场景。另一方面，人为定义的分类标准限制的相似性的标准。如在cifar10数据集下，和猫最相似的非猫样本应该是狗，因为它们都是动物、哺乳动物、家养动物，但是按照别的相似性标准来说，可能可以认为二者是相当不相似的：猫吃鱼，有的鸟也吃鱼，所以猫和鸟相似，而猫与狗不相似。不同的分类依据会得到截然不同的模型实例。
2. 模型的构建开销大。在cifar10的10类数据下，就要用到9个二分类CNN神经网络，而神经网络的训练、预测的开销可能很大。在数据量庞大、分类复杂的情况下，构建一个这种模型需要大量的开销。

总的来说，我认为这种基于变长前缀编码的相似性检索，在数据类别较少时能够由较好的效果，而在数据量庞大且数据类型极多的情况下，其实现就较为不理想。  
